# -*- coding: utf-8 -*-
"""LVADSUSR160_Keerthik_Vishal_Final_Assessment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XrbR2MWce8rXNw49X6Ib5I85R7lvXP_f
"""

#1.
#a)Import the dataset using pandas

import pandas as pd
df = pd.read_csv("Final Dataset - IPL.csv", usecols=["date"])

#1.
#b)Examine the basic information from the dataset using pandas functions to understand its structure and content(eg: number of rows and columns, types of data[numerical,categorical],checking for missing values, etc)

import pandas as pd
df = pd.read_csv('Final Dataset - IPL.csv')
print("Number of rows: ", df.shape[0])
print("Number of columns: ", df.shape[1])
print("Data types of columns:")
print(df.dtypes)
print("Missing values:")
print(df.isna().sum())
print("Last 5 rows:")
print(df.tail())

#2.
#a) Check and handle missing values:Assess the extent of missing data anddecide whether to fill or drop them, providing reasons for your decision

import pandas as pd
df = pd.read_csv('Final Dataset - IPL.csv')
print(df.isnull().sum())
df['match_winner'].fillna('N/A', inplace=True)
print(df.head())

#2.
#b) Identify and resolve ducplicate data entries: Ensure the dataset's integrity by removing any duplicates
import pandas as pd
df = pd.read_csv('Final Dataset - IPL.csv')
duplicates = df.duplicated()
print("Number of duplicate rows: ", duplicates.sum())
df.drop_duplicates(inplace=True)
print("Number of rows after cleaning: ", df.shape[0])

#3.
#a)
import pandas as pd

# Read the dataset
df = pd.read_csv('/content/Final Dataset - IPL.csv')
df['runs_scored'] = df['first_ings_score'] + df['second_ings_score']
mean_runs = df['runs_scored'].mean()
median_runs = df['runs_scored'].median()
mode_runs = df['runs_scored'].mode()[0]
range_runs = df['runs_scored'].max() - df['runs_scored'].min()
variance_runs = df['runs_scored'].var()
std_dev_runs = df['runs_scored'].std()
print(f"Mean runs scored: {mean_runs:.2f}")
print(f"Median runs scored: {median_runs:.2f}")
print(f"Mode runs scored: {mode_runs:.2f}")
print(f"Range of runs scored: {range_runs:.2f}")
print(f"Variance of runs scored: {variance_runs:.2f}")
print(f"Standard deviation of runs scored: {std_dev_runs:.2f}")

#4.
#a)
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/Final Dataset - IPL.csv')
df['runs_scored'] = df['first_ings_score'] + df['second_ings_score']
df['runs_scored'].hist()
plt.title("Runs Scored Distribution")
plt.show()
df['balls_faced'].hist()
plt.title("Balls Faced Distribution")
plt.show()
df.plot(kind='scatter', x='runs_scored', y='balls_faced', alpha=0.5)
plt.title("Runs Scored vs Balls Faced")
plt.show()
df.boxplot(column='runs_scored', by='batsman')
plt.title("Runs Scored by Batsmen")
plt.show()
teams = df['team'].value_counts()
teams.plot(kind='bar')
plt.title("Number of Matches Played by Each Team")
plt.show()
df['result'].value_counts().plot(kind='pie', autopct='%1.1f%%')
plt.title("Match Results")
plt.show()

#5.
#a)
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('/content/Final Dataset - IPL.csv')
df['first_ings_score'] = pd.to_numeric(df['first_ings_score'], errors='coerce')
df['second_ings_score'] = pd.to_numeric(df['second_ings_score'], errors='coerce')
df['runs_scored'] = df['first_ings_score'] + df['second_ings_score']
df = df.select_dtypes(include=['float64', 'int64'])
correlation_matrix = df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of IPL Dataset Variables')
plt.show()

#6.
#a)

"""Outliers are extreme values in a dataset that differ significantly from other observations. They can be caused by measurement or input error, data corruption, or true outlier observations. Identifying and handling outliers is important because they can skew the results of data analyses and hamper model performance.
To identify outliers in the given dataset /content/Final Dataset - IPL.csv, you can use statistical techniques such as the IQR method or z-scores.  If the outliers are the result of errors or are not representative of the data, you can remove them. If the outliers are important but are affecting the analysis, you can handle them by transforming the data or using robust statistical methods that are less sensitive to outliers.

"""

#7.
#a)
import pandas as pd
import pandas as pd
df = pd.read_csv('/content/Final Dataset - IPL.csv')
print(df.columns)
df = df.rename(columns={'team1': 'team_a', 'team2': 'team_b', 'city': 'venue'})
df['runs_scored'] = df['first_ings_score'] + df['second_ings_score']
team_performance = df.groupby(['team_a', 'team_b', 'venue']).agg({'runs_scored': 'mean', 'result': 'count'}).reset_index()
individual_performance = df.groupby('batsman').agg({'runs_scored': 'sum', 'balls_faced': 'sum', 'fours': 'sum', 'sixes': 'sum'}).reset_index()
specific_conditions = df.groupby(['venue', 'stage']).agg({'result': 'mean'}).reset_index()
print("Team Performance across Different Venues:")
print(team_performance.head())
print("\nIndividual Performance:")
print(individual_performance.head())
print("\nEffect of Specific Conditions on Outcomes:")
print(specific_conditions.head())

#8.
#a)
import pandas as pd
df = pd.read_csv('/content/Final Dataset - IPL.csv')
player_of_match = df['player_of_match'].value_counts().idxmax()
top_scorers = df.groupby('batsman')['runs_scored'].sum().nlargest(5)
best_bowlers = df.groupby('bowler')['wickets_taken'].sum().nlargest(5)
print(f"Player who often wins the Player of the Match: {player_of_match}")
print("\nTop Scorers:")
print(top_scorers)
print("\nBest Bowlers:")
print(best_bowlers)