# -*- coding: utf-8 -*-
"""LVADUSR160_Final Assessment_Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IbDHQZGSI0wj1aJZwsU1Aw6MgVtEE3nf
"""

import pandas as pd

df_fare = pd.read_csv('/content/Fare prediction.csv')

df_fare.info()

df_fare.describe()

df_fare.head(1)

from matplotlib import pyplot as plt
import seaborn as sns
def _plot_series(series, series_name, series_index=0):
  palette = list(sns.palettes.mpl_palette('Dark2'))
  xs = series['passenger count']
  ys = series['fare_amount']

fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')
df_sorted = df_fare.sort_values('fare_amount', ascending=True)
_plot_series(df_sorted, '')
sns.despine(fig=fig, ax=ax)
plt.xlabel('fare_amount')

df_fare.isnull().sum()

df_fare.bfill(inplace=True)

df_fare.isnull().sum()

duplicates = df_fare.duplicated(keep=False)
df_fare['dup_bool'] = duplicates
print(df_fare[df_fare['dup_bool'] == True].count())
df_fare.drop('dup_bool',axis=1)
df_fare.head(1)

from sklearn.preprocessing import LabelEncoder

lbl_enc = LabelEncoder()

from sklearn.model_selection import train_test_split

x = df_fare.drop(['key','pickup_datetime','pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude','passenger_count'],axis=1)
y = df_fare['fare_amount']

X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.33, random_state=42)

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
import xgboost as xgb
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import numpy as np

dec_reg = DecisionTreeRegressor()
dec_reg.fit(X_train,y_train)
y_pred = dec_reg.predict(X_test)
MSE = mean_squared_error(y_test, y_pred)
RMSE = np.sqrt(MSE)
r2 = r2_score(y_test, y_pred)
print("Decision tree \nMSE:", MSE)
print("RMSE:", RMSE)
print("R2 score:", r2)

lr_reg = LinearRegression()
lr_reg.fit(X_train,y_train)
y_pred = lr_reg.predict(X_test)
MSE = mean_squared_error(y_test, y_pred)
RMSE = np.sqrt(MSE)
r2 = r2_score(y_test, y_pred)
print("Linear regression \nMSE:", MSE)
print("RMSE:", RMSE)
print("R2 score:", r2)

RF_reg = RandomForestRegressor()
RF_reg.fit(X_train,y_train)
y_pred = RF_reg.predict(X_test)
MSE = mean_squared_error(y_test, y_pred)
RMSE = np.sqrt(MSE)
r2 = r2_score(y_test, y_pred)
print("Random Forest \nMSE:", MSE)
print("RMSE:", RMSE)
print("R2 score:", r2)

xgb_reg = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
xgb_reg.fit(X_train, y_train)
y_pred = xgb_reg.predict(X_test)
MSE = mean_squared_error(y_test, y_pred)
RMSE = np.sqrt(MSE)
r2 = r2_score(y_test, y_pred)
print("XGB \nMSE:", MSE)
print("RMSE:", RMSE)
print("R2 score:", r2)

# It is observed that random forest has least errors