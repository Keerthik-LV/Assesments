# -*- coding: utf-8 -*-
"""LVADUSR160_Reassessment_Cluster_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yxMEuCmeghw81mfAKkLadZO06zVE9I52

**1. Loading the dataset**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import seaborn as sns

df = pd.read_csv('/content/Credit Card Customer Data.csv')
df

"""**2. Data Pre-processing**

**Checking null values**
"""

df.isnull().sum()

num_cols = df.select_dtypes(include=['int64','float64']).columns
num_cols

"""**To fill the missing value detected using imputation**"""

from sklearn.impute import KNNImputer

imputing = KNNImputer()
for i in num_cols:
  df[i] = imputing.fit_transform(df[[i]])

df.isnull().sum()

df

"""**Checking for duplicates**"""

df.duplicated().sum()

"""**Comprehensive overview of data**"""

df.describe(include='all')

"""**3. Exploratory Data Analysis**

**Checking for univariate analysis and checking for the presence of outliers using the box plot**
"""

for i in num_cols:
  sns.boxplot(df[i])
  plt.show()

"""**Removal of outliers**"""

Q1 = df[num_cols].quantile(0.25)
Q3 = df[num_cols].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outlier = ((df[num_cols]<lower_bound )| (df[num_cols]>upper_bound)).any(axis=1)
df = df[~outlier]

for i in num_cols:
  sns.boxplot(df[i])
  plt.show()

"""**Checking the number of rows after removing outliers**"""

df.shape

"""**Univariate analysis**"""

numerical = df.select_dtypes(include=['number']).columns

for i in numerical:
  sns.histplot(df[i])
  plt.show()

"""**Understanding the correlation between variables in the data for bivaraiate analysis and feature selection**"""

df[numerical].corr()

sns.heatmap(df[numerical].corr(),annot=True,cmap='Greens',fmt=".2f")

df.nunique()

"""**Feature Selection - Drop the columns with no unique values and no correlation**




"""

nunique = df.nunique()
constant_columns = nunique[nunique <= 1].index.tolist()
df = df.drop(columns=constant_columns)
df

"""**Dropping Unique ID columns because they do not provide meaningful information about the patterns or relationships in the data.**

"""

#df = df.drop(columns='ID')

categ_cols = df.select_dtypes(include='object').columns
categ_cols

"""**Encoding categorical variables**"""

#from sklearn.preprocessing import LabelEncoder
#encoding = LabelEncoder()

#Encoding is not needed as there is no categorical values

"""**Scaling for better optimization**"""

scaling = StandardScaler()
scaled_data = scaling.fit_transform(df)
df

"""**4. Model training and testing**"""

elbow=[]
silhouette_scores = []
k_values = range(2, 10)
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    elbow.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(scaled_data, kmeans.labels_))

plt.plot(k_values, elbow, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia(WCSS)')
plt.title('Elbow Curve for Optimal k')
plt.xticks(k_values)
plt.show()

optimal_k = 3 #from the eblow point in the plot

# fit the KMeans model with this optimal k value

kmeans = KMeans(n_clusters=optimal_k, random_state=42)
kmeans.fit(scaled_data)
cluster_labels = kmeans.predict(scaled_data)

"""**5. Evaluation Metrics**

**Comparision of clusters using the silhoutte score**
"""

silhouette = silhouette_score(scaled_data, cluster_labels)
silhouette

"""**Cluster Profiling**"""

df['Cluster'] = kmeans.labels_

cluster_profiles = df.groupby('Cluster').mean()
print(cluster_profiles)

"""**Scatter plot to determine the effectiveness of the clustering done**

"""

plt.figure(figsize=(8, 6))
plt.scatter(scaled_data[:, 0], scaled_data[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.5)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('KMeans Clustering')

# Plotting centroids
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='o', c='red', s=200, label='Centroids')
plt.legend()
plt.colorbar(label='Cluster')
plt.show()

"""**6. Business recommendations**

**Insights:**

**Data Loading and Pre-processing:**

The dataset has 10,000 rows and 18 columns.
There were no missing values or duplicates in the dataset after imputation and deduplication.
Outlier removal was performed to ensure data quality.

**Exploratory Data Analysis:**

Univariate analysis using histograms showed the distribution of each feature.

Bivariate analysis using a correlation heatmap revealed relationships between features.

Feature selection was done based on the correlation matrix and the number of unique values in each column.
Categorical variables were not present in the dataset, so encoding was not required.

Data scaling was performed using StandardScaler to ensure better optimization during clustering.

**Clustering Model:**

The optimal number of clusters (k) was determined using the elbow method and silhouette score.
K-Means clustering was performed on the scaled data, resulting in 3 clusters.

The silhouette score of 0.54 indicates a moderately good clustering structure.

**Cluster Profiling:**

Cluster 0: Customers with average credit limit and balance, low purchases, and low payments.

Cluster 1: Customers with high credit limit and balance, high purchases, and high payments.

Cluster 2: Customers with low credit limit and balance, low purchases, and low payments.

**Business Recommendations:**

1. Identify high-risk customers (Cluster 1) and implement appropriate credit risk management strategies to mitigate potential losses.

2. Regularly monitor the spending and payment behavior of high-risk customers to detect any anomalies or signs of financial distress.
"""

